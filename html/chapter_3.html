<html lang="en">
<head>
<title>3 Memory</title>
<meta content="text/html; charset=utf-8" http-equiv="default-style"/>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:28e15094-8b6c-42d2-9184-6ba334c47321" name="Adept.expected.resource"/>
</head>
<body>
<div class="body">
<p class="sp"> </p>
<section aria-labelledby="ch3"role="doc-chapter">
<header>
<p class="bor-top"/>
<h1 class="chapter-number" id="ch3"><span aria-label="45" id="pg_45" role="doc-pagebreak"/><samp class="SANS_Helvetica_LT_Std_Bold_B_11">3</samp>       <samp class="SANS_Helvetica_LT_Std_Bold_B_11">Memory</samp></h1>
</header>
<blockquoterole="doc-epigraph">
<p class="EP1">It’s a poor sort of memory that only works backward.</p>
<p class="EPA1">—Lewis Carroll (1832–1898)</p>
</blockquote>
<p class="noindent">Consider the high-level operation <img alt="" class="inline" height="13" src="../images/3-C1.png" width="61"/> In chapter 2 we showed how logic gates can be utilized for representing numbers and for computing simple arithmetic expressions like <img alt="" class="inline" height="13" src="../images/3-C2.png" width="39"/> We now turn to discuss how logic gates can be used to <i>store values over time</i>—in particular, how a variable like <samp class="SANS_Consolas_Regular_11">x</samp> can be set to “contain” a value and persist it until we set it to another value. To do so, we’ll develop a new family of <i>memory chips</i>.</p>
<p>So far, all the chips that we built in chapters 1 and 2, culminating with the ALU, were time independent. Such chips are sometimes called <i>combinational</i>: they respond to different combinations of their inputs without delay, except for the time it takes their inner chip-parts to complete the computation. In this chapter we introduce and build <i>sequential</i> chips. Unlike combinational chips, which are oblivious to time, the outputs of sequential chips depend not only on the inputs in the current time but also on inputs and outputs that have been processed previously.</p>
<p>Needless to say, the notions of <i>current</i> and <i>previous</i> go hand in hand with the notion of <i>time</i>: you remember now what was committed to memory before. Thus, before we start talking about memory, we must first figure out how to use logic to model the progression of time. This can be done using a <i>clock</i> that generates an ongoing train of binary signals that we call <i>tick</i> and <i>tock</i>. The time between the beginning of a tick and the end of the subsequent tock is called a <i>cycle</i>, and these cycles will be used to regulate the operations of all the memory chips used by the computer.</p>
<p>Following a brief, user-oriented introduction to memory devices, we will present the art of sequential logic, which we will use for building time-dependent chips. We will then set out to build registers, RAM devices, and counters. These memory devices, along with the arithmetic devices built in chapter 2, comprise all the chips needed for building a complete, general-purpose computer system—a challenge that we will take up in chapter 5.</p>
<section>
<h2 class="head a-head"><span aria-label="46" id="pg_46" role="doc-pagebreak"/><b>3.1    Memory Devices</b></h2>
<p class="noindent">Computer programs use variables, arrays, and objects—abstractions that persist data over time. Hardware platforms support this ability by offering memory devices that know how to <i>maintain state</i>. Because evolution gave humans a phenomenal electro-chemical memory system, we tend to take for granted the ability to remember things over time. However, this ability is hard to implement in classical logic, which is aware of neither time nor state. Thus, to get started, we must first find a way to model the progression of time and endow logic gates with the ability to maintain state and respond to time changes.</p>
<p>We will approach this challenge by introducing a clock and an elementary, time-dependent logic gate that can flip and flop between two stable states: representing 0 and representing 1. This gate, called <i>data flip-flop</i> (DFF), is the fundamental building block from which all memory devices will be built. In spite of its central role, though, the DFF is a low-profile, inconspicuous gate: unlike registers, RAM devices, and counters, which play prominent roles in computer architectures, DFFs are used implicitly, as low-level chip-parts embedded deep within other memory devices.</p>
<p>The fundamental role of the DFF is seen clearly in <a href="chapter_3.html#fig3-1" id="rfig3-1">figure 3.1</a>, where it serves as the foundation of the memory hierarchy that we are about to build. We will show how DFFs can be used to create 1-bit registers and how <i>n</i> such registers can be lashed together to create an <i>n</i>-bit register. Next, we’ll construct a RAM device containing an arbitrary number of such registers. Among other things, we’ll develop a means for <i>addressing</i>, that is, accessing by address, any randomly chosen register from the RAM directly and instantaneously.</p>
<figure class="IMG"><img alt="" id="fig3-1" src="../images/figure_3.1.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-1">Figure 3.1</a></b>    The memory hierarchy built in this chapter.</p></figcaption>
</figure>
<p>Before setting out to build these chips, though, we’ll present a methodology and tools that enable modeling the progression of time and maintaining state over time.</p>
</section>
<section>
<h2 class="head a-head"><b>3.2    Sequential Logic</b></h2>
<p class="noindent">All the chips discussed in chapters 1 and 2 were based on classical logic, which is time independent. In order to develop memory devices, we need to extend our gate logic with the ability to respond not only to input changes but also to the ticking of a clock: we remember the meaning of the word <i>dog</i> in time <i>t</i> since we remembered it in time <img alt="" class="inline" height="14" src="../images/3-3.png" width="32"/> all the way back to the point of time when we first committed it to memory. In order to develop this temporal ability to <i>maintain state</i>, we must extend our computer architecture with a time dimension and build tools that handle time using Boolean functions.</p>
<section>
<h3 class="head b-head"><b>3.2.1    Time Matters</b></h3>
<p class="noindent">So far in our Nand to Tetris journey, we have assumed that chips respond to their inputs instantaneously: you input 7, 2, and “subtract” into the ALU, and <span class="ellipsis">…</span> <i>poof!</i> the ALU output <span aria-label="47" id="pg_47" role="doc-pagebreak"/>becomes 5. In reality, outputs are always delayed, due to at least two reasons. First, the inputs of the chips don’t appear out of thin air; rather, the signals that represent them travel from the outputs of other chips, and this travel takes time. Second, the computations that chips perform also take time; the more chip-parts the chip has—the more elaborate its logic—the more time it will take for the chip’s outputs to emerge fully formed from the chip’s circuitry.</p>
<p>Thus <i>time</i> is an issue that must be dealt with. As seen at the top of <a href="chapter_3.html#fig3-2" id="rfig3-2">figure 3.2</a>, time is typically viewed as a metaphorical arrow that progresses relentlessly forward. This progression is taken to be continuous: between every two time-points there is another time-point, and changes in the world can be infinitesimally small. This notion of time, which is popular among philosophers and physicists, is too deep and mysterious for computer scientists. Thus, instead of viewing time as a continuous progression, we prefer to break it into fixed-length intervals, called <i>cycles</i>. This representation is discrete, resulting in cycle 1, cycle 2, cycle 3, and so on. Unlike the continuous arrow of time, which has an infinite granularity, the cycles are atomic and indivisible: changes in the world occur only during cycle transitions; within cycles, the world stands still.</p>
<figure class="IMG"><img alt="" id="fig3-2" src="../images/figure_3.2.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-2">Figure 3.2</a></b>    Discrete time representation: State changes (input and output values) are observed only during cycle transitions. Within cycles, changes are ignored.</p></figcaption>
</figure>
<p>Of course the world never stands still. However, by treating time discretely, we make a conscious decision to ignore continuous change. We are content to know the state of the world in cycle <i>n</i>, and then in cycle <img alt="" class="inline" height="14" src="../images/3-4.png" width="35"/> but <i>during</i> each cycle the state is assumed to be—well, we don’t care. When it comes to building computer architectures, this discrete view of time serves two important design objectives. First, it can be used for neutralizing the randomness associated with communications and computation time delays. Second, it can be used for synchronizing the operations of different chips across the system, as we’ll see later.</p>
<p>To illustrate, let’s focus on the bottom part of <a href="chapter_3.html#fig3-2">figure 3.2</a>, which tracks how a Not gate (used as an example) responds to arbitrarily chosen inputs. When we feed the gate with 1, it takes a while before the gate’s output stabilizes on 0. However, since the cycle duration is—<i>by design</i>—longer than the time delay, when we reach the cycle’s end, the gate <span aria-label="48" id="pg_48" role="doc-pagebreak"/>output has already stabilized on 0. Since we probe the state of the world only at cycle ends, we don’t get to see the interim time delays; rather, it appears as if we fed the gate with 0, and <i>poof!</i> the gate responded with 1. If we make the same observations at the end of each cycle, we can generalize that when a Not gate is fed with some binary input <i>x</i>, it instantaneously outputs Not (<i>x</i>).</p>
<p>Thoughtful readers have probably noticed that for this scheme to work, the <i>cycle’s length</i> must be longer than the maximal time delays that can occur in the system. Indeed, cycle length is one of the most important design parameters of any hardware platform: When planning a computer, the hardware engineer chooses a cycle length that meets two design objectives. On the one hand, the cycle should be sufficiently long to contain, and neutralize, any possible time delay; on the other hand, the shorter the cycle, the faster the computer: if things happen only during cycle transitions, then obviously things happen faster when the cycles are shorter. To sum up, the cycle length is chosen to be slightly longer than the maximal time delay in any chip in the system. Following the tremendous progress in switching technologies, we are now able to create cycles as tiny as a billionth of a second, achieving remarkable computer speed.</p>
<p>Typically, the cycles are realized by an oscillator that alternates continuously between two phases labeled 0<span class="symb">−</span>1, <i>low-high</i>, or <i>ticktock</i> (as seen in <a href="chapter_3.html#fig3-2">figure 3.2</a>). The elapsed time between the beginning of a tick and the end of the subsequent tock is called a <i>cycle</i>, and each cycle is taken to model one discrete time unit. The current clock phase (<i>tick</i> or <i>tock</i>) is represented by a binary signal. Using the hardware’s circuitry, the same master clock signal is simultaneously broadcast to every memory chip in the system. In every such chip, the clock input <span aria-label="49" id="pg_49" role="doc-pagebreak"/>is funneled to the lower-level DFF gates, where it serves to ensure that the chip will commit to a new state, and output it, only at the end of the clock cycle.</p>
</section>
<section>
<h3 class="head b-head"><b>3.2.2    Flip-Flops</b></h3>
<p class="noindent">Memory chips are designed to “remember”, or store, information over time. The low-level devices that facilitate this storage abstraction are named <i>flip-flop</i> gates, of which there are several variants. In Nand to Tetris we use a variant named <i>data flip-flop</i>, or DFF, whose interface includes a single-bit data input and a single-bit data output (see top of <a href="chapter_3.html#fig3-3" id="rfig3-3">figure 3.3</a>). In addition, the DFF has a clock input that feeds from the master clock’s signal. Taken together, the data input and the clock input enable the DFF to implement the simple time-based behavior <samp class="SANS_Consolas_Regular_11">out<img alt="" class="inline" height="14" src="../images/3-5.png" width="82"/></samp> where <samp class="SANS_Consolas_Regular_11">in</samp> and <samp class="SANS_Consolas_Regular_11">out</samp> are the gate’s input and output values, and <i>t</i> is the current time unit (from now on, we’ll use the terms “time unit” and “cycle” interchangeably). Let us not worry how this behavior is actually implemented. For now, we simply observe that at the end of each time unit, the DFF outputs the input value from the previous time unit.</p>
<figure class="IMG"><img alt="" id="fig3-3" src="../images/figure_3.3.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-3">Figure 3.3</a></b>    The data flip-flop (top) and behavioral example (bottom). In the first cycle the previous input is unknown, so the DFF’s output is undefined. In every subsequent time unit, the DFF outputs the input from the previous time unit. Following gate diagramming conventions, the clock input is marked by a small triangle, drawn at the bottom of the gate icon.</p></figcaption>
</figure>
<p>Like Nand gates, DFF gates lie deep in the hardware hierarchy. As shown in <a href="chapter_3.html#fig3-1">figure 3.1</a>, all the memory chips in the computer—registers, RAM units, and counters—are based, at bottom, on DFF gates. All these DFFs are connected to the same master clock, forming a huge distributed “chorus line.” At the end of each clock cycle, the outputs of <i>all</i> the DFFs in the computer commit to their inputs from the previous cycle. At all other times, the DFFs are <i>latched</i>, meaning that changes in their inputs have no immediate effect on their <span aria-label="50" id="pg_50" role="doc-pagebreak"/>outputs. This conduction operation effects any one of the system’s numerous DFF gates many times per second (depending on the computer’s clock frequency).</p>
<p>Hardware implementations realize the time dependency using a dedicated clock bus that feeds the master clock signal simultaneously to all the DFF gates in the system. Hardware simulators emulate the same effect in software. In particular, the Nand to Tetris hardware simulator features a clock icon, enabling the user to advance the clock interactively, as well as <samp class="SANS_Consolas_Regular_11">tick</samp> and <samp class="SANS_Consolas_Regular_11">tock</samp> commands that can be used programmatically, in test scripts.</p>
</section>
<section>
<h3 class="head b-head"><b>3.2.3    Combinational and Sequential Logic</b></h3>
<p class="noindent">All the chips that were developed in chapters 1 and 2, starting with the elementary logic gates and culminating with the ALU, were designed to respond only to changes that occur during the current clock cycle. Such chips are called <i>time-independent</i> chips, or <i>combinational</i> chips. The latter name alludes to the fact that these chips respond only to different combinations of their input values, while paying no attention to the progression of time.</p>
<p>In contrast, chips that are designed to respond to changes that occurred during previous time units (and possibly during the current time unit as well) are called <i>sequential</i>, or <i>clocked</i>. The most fundamental sequential gate is the DFF, and any chip that includes it, either directly or indirectly, is also said to be sequential. <a href="chapter_3.html#fig3-4" id="rfig3-4">Figure 3.4</a> depicts a typical sequential logic configuration. The main element in this configuration is a set of one or more chips that include DFF chip-parts, either directly or indirectly. As shown in the figure, these sequential chips may also interact with combinational chips. The feedback loop enables the sequential chip to respond to inputs and outputs from the previous time unit. In combi<span aria-label="51" id="pg_51" role="doc-pagebreak"/>national chips, where time is neither modeled nor recognized, the introduction of feedback loops is problematic: the output of the chip would depend on its input, which itself would depend on the output, and thus the output would depend on itself. Note, however, that there is no difficulty in feeding outputs back into inputs, as long as the feedback loop goes through a DFF gate: the DFF introduces an inherent time delay so that the output at time <i>t</i> does not depend on itself but rather on the output at time <img alt="" class="inline" height="12" src="../images/3-6.png" width="29"/></p>
<figure class="IMG"><img alt="" id="fig3-4" src="../images/figure_3.4.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-4">Figure 3.4</a></b>    Sequential logic design typically involves DFF gates that feed from, and connect to, combinational chips. This gives sequential chips the ability to respond to current as well as to previous inputs and outputs.</p></figcaption>
</figure>
<p>The time dependency of sequential chips has an important side effect that serves to synchronize the overall computer architecture. To illustrate, suppose we instruct the ALU to compute <img alt="" class="inline" height="12" src="../images/3-7.png" width="38"/> where <i>x</i> is the output of a nearby register, and <i>y</i> is the output of a remote RAM register. Because of physical constraints like distance, resistance, and interference, the electric signals representing <i>x</i> and <i>y</i> will likely arrive at the ALU at different times. However, being a combinational chip, the ALU is insensitive to the concept of time—it continuously and happily adds up whichever data values happen to lodge at its inputs. Thus, it will take some time before the ALU’s output stabilizes to the correct <img alt="" class="inline" height="12" src="../images/3-8.png" width="33"/> result. Until then, the ALU will generate garbage.</p>
<p>How can we overcome this difficulty? Well, if we use a discrete representation of time, <i>we simply don’t care</i>. All we have to do is ensure, when we build the computer’s clock, that the duration of the clock cycle will be slightly longer than the time it takes a bit to travel the longest distance from one chip to another, plus the time it takes to complete the most time-consuming within-chip calculation. This way, we are guaranteed that by the end of the clock cycle, the ALU’s output will be valid. This, in a nutshell, is the trick that turns a set of standalone hardware components into a well-synchronized system. We will have more to say about this master orchestration when we build the computer architecture in chapter 5.</p>
</section>
</section>
<section>
<h2 class="head a-head"><b>3.3    Specification</b></h2>
<p class="noindent">We now turn to specify the memory chips that are typically used in computer architectures:</p>
<ul class="List-1">
<li class="BLF">data flip-flops (DFFs)</li>
<li class="BL1">registers (based on DFFs)</li>
<li class="BL1">RAM devices (based on registers)</li>
<li class="BLL1">counters (based on registers)</li>
</ul>
<p class="TNI1">As usual, we describe these chips <i>abstractly</i>. In particular, we focus on each chip’s interface: inputs, outputs, and function. How the chips deliver this functionality will be discussed in the Implementation section.</p>
<section>
<h3 class="head b-head"><span aria-label="52" id="pg_52" role="doc-pagebreak"/><b>3.3.1    Data Flip-Flop</b></h3>
<p class="noindent">The most elementary sequential device that we will use—the basic component from which all other memory chips will be constructed—is the <i>data flip-flop</i>. A DFF gate has a single-bit data input, a single-bit data output, a clock input, and a simple time-dependent behavior: <samp class="SANS_Consolas_Regular_11">out<img alt="" class="inline" height="14" src="../images/3-9.png" width="88"/></samp></p>
<p class="STNI1"><b>Usage</b>: If we put a one-bit value in the DFF’s input, the DFF’s state will be set to this value, and the DFF’s output will emit it in the next time unit (see <a href="chapter_3.html#fig3-3">figure 3.3</a>). This humble operation will prove most useful in the implementation of registers, which is described next.</p>
</section>
<section>
<h3 class="head b-head"><b>3.3.2    Registers</b></h3>
<p class="noindent">We present a single-bit register, named <samp class="SANS_Consolas_Regular_11">Bit</samp>, and a 16-bit register, named <samp class="SANS_Consolas_Regular_11">Register</samp>. The <samp class="SANS_Consolas_Regular_11">Bit</samp> chip is designed to store a single bit of information—0 or 1—over time. The chip interface consists of an <samp class="SANS_Consolas_Regular_11">in</samp> input that carries a data bit, a <samp class="SANS_Consolas_Regular_11">load</samp> input that enables the register for writes, and an <samp class="SANS_Consolas_Regular_11">out</samp> output that emits the current state of the register. The <samp class="SANS_Consolas_Regular_11">Bit</samp> API and input/output behavior are described in <a href="chapter_3.html#fig3-5" id="rfig3-5">figure 3.5</a>.</p>
<figure class="IMG"><img alt="" id="fig3-5" src="../images/figure_3.5.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-5">Figure 3.5</a></b>    1-bit register. Stores and emits a 1-bit value until instructed to load a new value.</p></figcaption>
</figure>
<p><a href="chapter_3.html#fig3-5">Figure 3.5</a> illustrates how the single-bit register behaves over time, responding to arbitrary examples of <samp class="SANS_Consolas_Regular_11">in</samp> and <samp class="SANS_Consolas_Regular_11">load</samp> inputs. Note that irrespective of the input value, as long as the <samp class="SANS_Consolas_Regular_11">load</samp> bit is not asserted, the register is latched, maintaining its current state.</p>
<p><span aria-label="53" id="pg_53" role="doc-pagebreak"/>The 16-bit <samp class="SANS_Consolas_Regular_11">Register</samp> chip behaves exactly the same as the <samp class="SANS_Consolas_Regular_11">Bit</samp> chip, except that it is designed to handle 16-bit values. <a href="chapter_3.html#fig3-6" id="rfig3-6">Figure 3.6</a> gives the details.</p>
<figure class="IMG"><img alt="" id="fig3-6" src="../images/figure_3.6.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-6">Figure 3.6</a></b>    16-bit Register. Stores and emits a 16-bit value until instructed to load a new value.</p></figcaption>
</figure>
<p class="STNI1"><b>Usage</b>: The <samp class="SANS_Consolas_Regular_11">Bit</samp> register and the 16-bit <samp class="SANS_Consolas_Regular_11">Register</samp> are used identically. To read the state of the register, probe the value of <samp class="SANS_Consolas_Regular_11">out</samp>. To set the register’s state to <i>v</i>, put <i>v</i> in the <samp class="SANS_Consolas_Regular_11">in</samp> input, and assert (put 1 into) the <samp class="SANS_Consolas_Regular_11">load</samp> bit. This will set the register’s state to <i>v</i>, and, from the next time unit onward, the register will commit to the new value, and its <samp class="SANS_Consolas_Regular_11">out</samp> output will start emitting it. We see that the <samp class="SANS_Consolas_Regular_11">Register</samp> chip fulfills the classical function of a memory device: it remembers and emits the last value that was written into it, until we set it to another value.</p>
</section>
<section>
<h3 class="head b-head"><b>3.3.3    Random Access Memory</b></h3>
<p class="noindent">A direct-access memory unit, also called <i>Random Access Memory</i>, or RAM, is an aggregate of <i>n</i> <samp class="SANS_Consolas_Regular_11">Register</samp> chips. By specifying a particular address (a number between 0 to <img alt="" class="inline" height="12" src="../images/3-10.png" width="29"/>), each register in the RAM can be selected and made available for read/write operations. Importantly, the access time to any randomly selected memory register is instantaneous and independent of the register’s address and the size of the RAM. That’s what makes RAM devices so remarkably useful: even if they contain billions of registers, we can still access and manipulate each selected register directly, in the same instantaneous access time. The RAM API is given in <a href="chapter_3.html#fig3-7" id="rfig3-7">figure 3.7</a>.</p>
<figure class="IMG"><img alt="" id="fig3-7" src="../images/figure_3.7.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-7">Figure 3.7</a></b>    A RAM chip, consisting of <i>n</i> 16-bit <samp class="SANS_Consolas_Regular_11">Register</samp> chips that can be selected and manipulated separately. The register addresses <img alt="" class="inline" height="13" src="../images/3-11.png" width="63"/> are not part of the chip hardware. Rather, they are realized by a gate logic implementation that will be discussed in the next section.</p></figcaption>
</figure>
<p class="STNI1"><b>Usage</b>: To read the contents of register number <i>m</i>, set the <samp class="SANS_Consolas_Regular_11">address</samp> input to <i>m</i>. This action will select register number <i>m</i>, and the RAM’s output will emit its value. To write a new value <i>v</i> into register number <i>m</i>, set the <samp class="SANS_Consolas_Regular_11">address</samp> input to <i>m</i>, set the <samp class="SANS_Consolas_Regular_11">in</samp> input to <i>v</i>, and assert the <samp class="SANS_Consolas_Regular_11">load</samp> bit (set it to 1). This action will select register number <i>m</i>, enable it for writing, and set its value to <i>v</i>. In the next time unit, the RAM’s output will start emitting <i>v</i>.</p>
<p>The net result is that the RAM device behaves exactly as required: a bank of addressable registers, each of which can be accessed, and operated upon, independently. In the case of a read operation (<samp class="SANS_Consolas_Regular_11">load</samp><span class="code_eq-symb">==</span><samp class="SANS_Consolas_Regular_11">0</samp>), the RAM’s output immediately emits the value of the selected register. In the case of a write operation (<samp class="SANS_Consolas_Regular_11">load</samp><span class="code_eq-symb">==</span><samp class="SANS_Consolas_Regular_11">1</samp>), the selected memory register is set to the input value, and the RAM’s output will start emitting it from the next time unit onward.</p>
<p><span aria-label="54" id="pg_54" role="doc-pagebreak"/>Importantly, the RAM implementation must ensure that the access time to <i>any</i> register in the RAM will be nearly instantaneous. If this were not the case, we would not be able to fetch instructions and manipulate variables in a reasonable time, making computers impractically slow. The magic of instantaneous access time will be unfolded shortly, in the Implementation section.</p>
</section>
<section>
<h3 class="head b-head"><b>3.3.4    Counter</b></h3>
<p class="noindent">The Counter is a chip that knows how to increment its value by 1 each time unit. When we build our computer architecture in chapter 5, we will call this chip <samp class="SANS_Consolas_Regular_11">Program</samp> <samp class="SANS_Consolas_Regular_11">Counter</samp>, or <samp class="SANS_Consolas_Regular_11">PC</samp>, so that’s the name that we will use here also.</p>
<p>The interface of our <samp class="SANS_Consolas_Regular_11">PC</samp> chip is identical to that of a register, except that it also has control bits labeled <samp class="SANS_Consolas_Regular_11">inc</samp> and <samp class="SANS_Consolas_Regular_11">reset</samp>. When <samp class="SANS_Consolas_Regular_11">inc</samp><span class="code_eq-symb">==</span><samp class="SANS_Consolas_Regular_11">1</samp>, the counter increments its state in every clock cycle, effecting the operation <samp class="SANS_Consolas_Regular_11">PC</samp><span class="symb-code">++</span>. If we want to reset the counter to 0, we assert the <samp class="SANS_Consolas_Regular_11">reset</samp> bit; if we want to set the counter to the value <i>v</i>, we put <i>v</i> in the <samp class="SANS_Consolas_Regular_11">in</samp> input and assert the <samp class="SANS_Consolas_Regular_11">load</samp> bit, as we normally do with registers. The details are given in <a href="chapter_3.html#fig3-8" id="rfig3-8">figure 3.8</a>.</p>
<figure class="IMG"><img alt="" id="fig3-8" src="../images/figure_3.8.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-8">Figure 3.8</a></b>    Program Counter (<samp class="SANS_Consolas_Regular_11">PC</samp>): To use it properly, at most one of the <samp class="SANS_Consolas_Regular_11">load</samp>, <samp class="SANS_Consolas_Regular_11">inc</samp>, or <samp class="SANS_Consolas_Regular_11">reset</samp> bits should be asserted.</p></figcaption>
</figure>
<p class="STNI1"><b>Usage</b>: To read the current contents of the <samp class="SANS_Consolas_Regular_11">PC</samp>, probe the <samp class="SANS_Consolas_Regular_11">out</samp> pin. To reset the <samp class="SANS_Consolas_Regular_11">PC</samp>, assert the <samp class="SANS_Consolas_Regular_11">reset</samp> bit and set the other control bits to <samp class="SANS_Consolas_Regular_11">0</samp>. To have the <samp class="SANS_Consolas_Regular_11">PC</samp> increment by <samp class="SANS_Consolas_Regular_11">1</samp> in each time unit until further notice, assert the <samp class="SANS_Consolas_Regular_11">inc</samp> bit and set the other control bits to <samp class="SANS_Consolas_Regular_11">0</samp>. To set the <samp class="SANS_Consolas_Regular_11">PC</samp> to the value <i>v</i>, set the <samp class="SANS_Consolas_Regular_11">in</samp> input to <i>v</i>, assert the <samp class="SANS_Consolas_Regular_11">load</samp> bit, and set the other control bits to <samp class="SANS_Consolas_Regular_11">0</samp>.</p>
</section>
</section>
<section>
<h2 class="head a-head"><b>3.4    Implementation</b></h2>
<p class="noindent">The previous section presented a family of memory chip abstractions, focusing on their interface and functionality. This section focuses on how these chips can be realized, using <span aria-label="55" id="pg_55" role="doc-pagebreak"/>simpler chips that were already built. As usual, our implementation guidelines are intentionally suggestive; we want to give you enough hints to complete the implementation yourself, using HDL and the supplied hardware simulator.</p>
<section>
<h3 class="head b-head"><b>3.4.1    Data Flip-Flop</b></h3>
<p class="noindent">A DFF gate is designed to be able to “flip-flop” between two stable states, representing 0 and representing 1. This functionality can be implemented in several different ways, including ones that use Nand gates only. The Nand-based DFF implementations are elegant, yet intricate and impossible to model in our hardware simulator since they require feedback loops among combinational gates. Wishing to abstract away this complexity, we will treat the DFF as a primitive building block. In particular, the Nand to Tetris hardware simulator provides a built-in DFF implementation that can be readily used by other chips, as we now turn to describe.</p>
</section>
<section>
<h3 class="head b-head"><b>3.4.2    Registers</b></h3>
<p class="noindent">Register chips are memory devices: they are expected to implement the basic behavior <samp class="SANS_Consolas_Regular_11">out<img alt="" class="inline" height="14" src="../images/3-12C.png" width="92"/>,</samp> remembering and emitting their state over time. This looks similar to the DFF behavior, which is <samp class="SANS_Consolas_Regular_11">out<img alt="" class="inline" height="14" src="../images/3-13C.png" width="81"/>.</samp> If we could only feed the DFF output back into its input, this could be a good starting point for implementing the one-bit <samp class="SANS_Consolas_Regular_11">Bit</samp> register. This solution is shown on the left of <a href="chapter_3.html#fig3-9" id="rfig3-9">figure 3.9</a>.</p>
<figure class="IMG"><img alt="" id="fig3-9" src="../images/figure_3.9.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig3-9">Figure 3.9</a></b>    The <samp class="SANS_Consolas_Regular_11">Bit</samp> (1-bit register) implementation: invalid (left) and correct (right) solutions.</p></figcaption>
</figure>
<p>It turns out that the implementation shown on the left of <a href="chapter_3.html#fig3-9">figure 3.9</a> is invalid, for several related reasons. First, the implementation does not expose a <samp class="SANS_Consolas_Regular_11">load</samp> bit, as required by the register’s interface. Second, there is no way to tell the <samp class="SANS_Consolas_Regular_11">DFF</samp> chip-part when to draw its input from the <samp class="SANS_Consolas_Regular_11">in</samp> wire and when from the incoming <samp class="SANS_Consolas_Regular_11">out</samp> value. Indeed, HDL programming rules forbid feeding a pin from more than one source.</p>
<p>The good thing about this invalid design is that it leads us to the correct implementation, shown on the right of <a href="chapter_3.html#fig3-9">figure 3.9</a>. As the chip diagram shows, a natural way to resolve the input ambiguity is introducing a multiplexer into the design. The <i>load bit</i> of the overall register chip can then be funneled to the <i>select bit</i> of the inner multiplexer: If we set this bit to 1, the multiplexer will feed the <samp class="SANS_Consolas_Regular_11">in</samp> value into the <samp class="SANS_Consolas_Regular_11">DFF</samp>; if we set the <samp class="SANS_Consolas_Regular_11">load</samp> bit to 0, the <span aria-label="56" id="pg_56" role="doc-pagebreak"/>multiplexer will feed the <samp class="SANS_Consolas_Regular_11">DFF</samp>’s previous output. This will yield the behavior “if <samp class="SANS_Consolas_Regular_11">load</samp>, set the register to a new value, else set it to the previously stored value”—exactly how we want a register to behave.</p>
<p>Note that the feedback loop just described does not entail cyclical <i>data race</i> problems: the loop goes through a <samp class="SANS_Consolas_Regular_11">DFF</samp> gate, which introduces a time delay. In fact, the <samp class="SANS_Consolas_Regular_11">Bit</samp> design shown in <a href="chapter_3.html#fig3-9">figure 3.9</a> is a special case of the general sequential logic design shown in <a href="chapter_3.html#fig3-4">figure 3.4</a>.</p>
<p>Once we’ve completed the implementation of the single-bit <samp class="SANS_Consolas_Regular_11">Bit</samp> register, we can move on to constructing a <i>w</i>-bit register. This can be achieved by forming an array of <i>w</i> <samp class="SANS_Consolas_Regular_11">Bit</samp> chips (see <a href="chapter_3.html#fig3-1">figure 3.1</a>). The basic design parameter of such a register is <i>w—</i>the number of bits that it is supposed to hold—for example, 16, 32, or 64. Since the Hack computer will be based on a 16-bit hardware platform, our <samp class="SANS_Consolas_Regular_11">Register</samp> chip will be based on sixteen <samp class="SANS_Consolas_Regular_11">Bit</samp> chip-parts.</p>
<p>The <samp class="SANS_Consolas_Regular_11">Bit</samp> register is the only chip in the Hack architecture that uses a <samp class="SANS_Consolas_Regular_11">DFF</samp> gate directly; all the higher-level memory devices in the computer use <samp class="SANS_Consolas_Regular_11">DFF</samp> chips indirectly, by virtue of using <samp class="SANS_Consolas_Regular_11">Register</samp> chips made of <samp class="SANS_Consolas_Regular_11">Bit</samp> chips. Note that the inclusion of a <samp class="SANS_Consolas_Regular_11">DFF</samp> gate in the design of any chip—directly or indirectly—turns the latter chip, as well as all the higher-level chips that use it as a chip-part, into time-dependent chips.</p>
</section>
<section>
<h3 class="head b-head"><b>3.4.3    RAM</b></h3>
<p class="noindent">The Hack hardware platform requires a RAM device of 16K (16384) 16-bit registers, so that’s what we have to implement. We propose the following gradual implementation roadmap:</p>
<figure class="IMG-L"><img alt="" class="img100" src="../images/figure_wo_caption_3.1.png"/></figure>
<p class="TNI1"><span aria-label="57" id="pg_57" role="doc-pagebreak"/>All these memory chips have precisely the same <samp class="SANS_Consolas_Regular_11-SC">RAM</samp><i>n</i> API given in <a href="chapter_3.html#fig3-7">figure 3.7</a>. Each RAM chip has <i>n</i> registers, and the width of its address input is <img alt="" class="inline" height="15" src="../images/57.png" width="59"/> bits. We now describe how these chips can be implemented, starting with <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp>.</p>
<p>A <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp> chip features 8 registers, as shown in <a href="chapter_3.html#fig3-7">figure 3.7</a>, for <img alt="" class="inline" height="12" src="../images/3-14.png" width="32"/>. Each register can be selected by setting the <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp>’s 3-bit address input to a value between 0 and 7. The act of <i>reading</i> the value of a selected register can be described as follows: Given some <samp class="SANS_Consolas_Regular_11">address</samp> (a value between 0 and 7), how can we “select” register number <samp class="SANS_Consolas_Regular_11">address</samp> and pipe its output to the <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp>’s output? <i>Hint:</i> We can do it using one of the combinational chips built in project 1. That’s why reading the value of a selected RAM register is achieved nearly instantaneously, independent of the clock and of the number of registers in the RAM. In a similar way, the act of <i>writing</i> a value into a selected register can be described as follows: Given an <samp class="SANS_Consolas_Regular_11">address</samp> value, a <samp class="SANS_Consolas_Regular_11">load</samp> value (<samp class="SANS_Consolas_Regular_11">1</samp>), and a 16-bit <samp class="SANS_Consolas_Regular_11">in</samp> value, how can we set the value of register number <samp class="SANS_Consolas_Regular_11">address</samp> to <samp class="SANS_Consolas_Regular_11">in</samp>? <i>Hint:</i> The 16-bit <samp class="SANS_Consolas_Regular_11">in</samp> data can be fed simultaneously to the <samp class="SANS_Consolas_Regular_11">in</samp> inputs of all eight <samp class="SANS_Consolas_Regular_11">Register</samp> chips. Using another combinational chip developed in project 1, along with the <samp class="SANS_Consolas_Regular_11">address</samp> and <samp class="SANS_Consolas_Regular_11">load</samp> inputs, you can ensure that only one of the registers will accept the incoming <samp class="SANS_Consolas_Regular_11">in</samp> value, while all the other seven registers will ignore it.</p>
<p>We note in passing that the RAM registers are not marked with addresses in any physical sense. Rather, the logic described above is capable of, and sufficient for, selecting individual registers according to their address, and this is done by virtue of using combinational chips. Now here is a crucially important observation: since combinational logic is time independent, the access time to any individual register will be nearly instantaneous.</p>
<p>Once we’ve implemented the <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp> chip, we can move on to implementing a <samp class="SANS_Consolas_Regular_11-SC">RAM64</samp> chip. The implementation can be based on eight <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp> chip-parts. To select a particular register from the <samp class="SANS_Consolas_Regular_11-SC">RAM64</samp> memory, we use a 6-bit address, say <i>xxxyyy</i>. The <i>xxx</i> bits can be used to select one of the <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp> chips, and the <i>yyy</i> bits can be used to select one of the registers within the selected <samp class="SANS_Consolas_Regular_11-SC">RAM8</samp>. This hierarchical addressing scheme can be effected by gate logic. The same implementation idea can guide the implementation of the remaining <samp class="SANS_Consolas_Regular_11-SC">RAM512</samp>, <samp class="SANS_Consolas_Regular_11-SC">RAM4K</samp>, and <samp class="SANS_Consolas_Regular_11-SC">RAM16K</samp> chips.</p>
<p>To recap, we take an aggregate of an unlimited number of registers, and impose on it a combinational superstructure that permits direct access to any individual register. We hope that the beauty of this solution does not escape the reader’s attention.</p>
</section>
<section>
<h3 class="head b-head"><b>3.4.4    Counter</b></h3>
<p class="noindent">A counter is a memory device that can increment its value in every time unit. In addition, the counter can be set to 0 or some other value. The basic storage and counting functionalities of the counter can be implemented, respectively, by a <samp class="SANS_Consolas_Regular_11">Register</samp> chip and by the incrementer chip built in project 2. The logic that selects between the counter’s <samp class="SANS_Consolas_Regular_11">inc</samp>, <samp class="SANS_Consolas_Regular_11">load</samp>, and <samp class="SANS_Consolas_Regular_11">reset</samp> modes can be implemented using some of the multiplexers built in project 1.</p>
</section>
</section>
<section>
<h2 class="head a-head"><span aria-label="58" id="pg_58" role="doc-pagebreak"/><b>3.5    Project</b></h2>
<p class="noindent"><b>Objective</b>: Build all the chips described in the chapter. The building blocks that you can use are primitive <samp class="SANS_Consolas_Regular_11">DFF</samp> gates, chips that you will build on top of them, and the gates built in previous chapters.</p>
<p class="STNI1"><b>Resources</b>: The only tool that you need for this project is the Nand to Tetris hardware simulator. All the chips should be implemented in the HDL language specified in appendix 2. As usual, for each chip we supply a skeletal <samp class="SANS_Consolas_Regular_11">.hdl</samp> program with a missing implementation part, a <samp class="SANS_Consolas_Regular_11">.tst</samp> script file that tells the hardware simulator how to test it, and a <samp class="SANS_Consolas_Regular_11">.cmp</samp> compare file that defines the expected results. Your job is to complete the missing implementation parts of the supplied <samp class="SANS_Consolas_Regular_11">.hdl</samp> programs.</p>
<p class="STNI1"><b>Contract</b>: When loaded into the hardware simulator, your chip design (modified <samp class="SANS_Consolas_Regular_11">.hdl</samp> program), tested on the supplied <samp class="SANS_Consolas_Regular_11">.tst</samp> file, should produce the outputs listed in the supplied <samp class="SANS_Consolas_Regular_11">.cmp</samp> file. If that is not the case, the simulator will let you know.</p>
<p class="STNI1"><b>Tip</b>: The data flip-flop (<samp class="SANS_Consolas_Regular_11">DFF</samp>) gate is considered primitive; thus there is no need to build it. When the simulator encounters a <samp class="SANS_Consolas_Regular_11">DFF</samp> chip-part in an HDL program, it automatically invokes the <samp class="SANS_Consolas_Regular_11">tools/builtIn/DFF.hdl</samp> implementation.</p>
<p class="STNI1"><b>Folders structure of this project</b>: When constructing RAM chips from lower-level RAM chip-parts, we recommend using built-in versions of the latter. Otherwise, the simulator will recursively generate numerous memory-resident software objects, one for each of the many chip-parts that make up a typical RAM unit. This may cause the simulator to run slowly or, worse, run out of the memory of the host computer on which the simulator is running.</p>
<p>To avert this problem, we’ve partitioned the RAM chips built in this project into two subfolders. The <samp class="SANS_Consolas_Regular_11-SC">RAM8.hdl</samp> and <samp class="SANS_Consolas_Regular_11-SC">RAM64.hdl</samp> programs are stored in <samp class="SANS_Consolas_Regular_11">projects/03/a</samp>, and the other, higher-level RAM chips are stored in <samp class="SANS_Consolas_Regular_11">projects/03/b</samp>. This partitioning is done for one purpose only: when evaluating the RAM chips stored in the <samp class="SANS_Consolas_Regular_11">b</samp> folder, the simulator will be forced to use built-in implementations of the <samp class="SANS_Consolas_Regular_11-SC">RAM64</samp> chip-parts, because <samp class="SANS_Consolas_Regular_11-SC">RAM64.hdl</samp> cannot be found in the <samp class="SANS_Consolas_Regular_11">b</samp> folder.</p>
<p class="STNI1"><b>Steps</b>: We recommend proceeding in the following order:</p>
<ol class="List-1">
<li class="NLF" value="1">The hardware simulator needed for this project is available in <samp class="SANS_Consolas_Regular_11">nand2tetris/tools</samp>.</li>
<li class="NL">Consult appendix 2 and the Hardware Simulator Tutorial, as needed.</li>
<li class="NLL">Build and simulate all the chips specified in the <samp class="SANS_Consolas_Regular_11">projects/03</samp> folder.</li>
</ol>
<p class="STNI1"><b>A web-based version of project 3</b> is available at <a href="http://www.nand2tetris.org">www<wbr/>.nand2tetris<wbr/>.org</a>.</p>
</section>
<section>
<h2 class="head a-head"><span aria-label="59" id="pg_59" role="doc-pagebreak"/><b>3.6    Perspective</b></h2>
<p class="noindent">The cornerstone of all the memory systems described in this chapter is the flip-flop, which we treated abstractly as a primitive, built-in gate. The usual approach is to construct flip-flops from elementary combinational gates (e.g., Nand gates) connected in feedback loops. The standard construction begins by building a non-clocked flip-flop which is bi-stable, that is, can be set to be in one of two states (storing 0, and storing 1). Then a clocked flip-flop is obtained by cascading two such non-clocked flip-flops, the first being set when the clock <i>ticks</i> and the second when the clock <i>tocks</i>. This master-slave design endows the overall flip-flop with the desired clocked synchronization functionality.</p>
<p>Such flip-flop implementations are both elegant and intricate. In this book we have chosen to abstract away these low-level considerations by treating the flip-flop as a primitive gate. Readers who wish to explore the internal structure of flip-flop gates can find detailed descriptions in most logic design and computer architecture textbooks.</p>
<p>One reason not to dwell on flip-flop esoterica is that the lowest level of the memory devices used in modern computers is not necessarily constructed from flip-flop gates. Instead, modern memory chips are carefully optimized, exploiting the unique physical properties of the underlying storage technology. Many such alternative technologies are available today to computer designers; as usual, which technology to use is a cost-performance issue. Likewise, the recursive ascent method that we used to build the RAM chips is elegant but not necessarily efficient. More efficient implementations are possible.</p>
<p>Aside from these physical considerations, all the chip constructions described in this chapter—the registers, the counter, and the RAM chips—are standard, and versions of them can be found in every computer system.</p>
<p>In chapter 5, we will use the register chips built in this chapter, along with the ALU built in chapter 2, to build a Central Processing Unit. The CPU will then be augmented with a RAM device, leading up to a general-purpose computer architecture capable of executing programs written in a machine language. This machine language is discussed in the next chapter.</p>
</section>
</section>
</div>
</body>
</html>