<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en" xml:lang="en">
<head>
<title>2 Boolean Arithmetic</title>
<meta content="text/html; charset=utf-8" http-equiv="default-style"/>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:28e15094-8b6c-42d2-9184-6ba334c47321" name="Adept.expected.resource"/>
</head>
<body epub:type="bodymatter">
<div class="body">
<p class="sp"> </p>
<section aria-labelledby="ch2" epub:type="chapter" role="doc-chapter">
<header>
<p class="bor-top"/>
<h1 class="chapter-number" id="ch2"><span aria-label="31" id="pg_31" role="doc-pagebreak"/><samp class="SANS_Helvetica_LT_Std_Bold_B_11">2</samp>       <samp class="SANS_Helvetica_LT_Std_Bold_B_11">Boolean Arithmetic</samp></h1>
</header>
<blockquote epub:type="epigraph" role="doc-epigraph">
<p class="EP1">Counting is the religion of this generation, its hope and salvation.</p>
<p class="EPA1">—Gertrude Stein (1874–1946)</p>
</blockquote>
<p class="noindent">In this chapter we build a family of chips designed to represent numbers and perform arithmetic operations. Our starting point is the set of logic gates built in chapter 1, and our ending point is a fully functional <i>Arithmetic Logic Unit</i>. The ALU will later become the computational centerpiece of the <i>Central Processing Unit</i> (CPU)—the chip that executes all the instructions handled by the computer. Hence, building the ALU is an important milestone in our Nand to Tetris journey.</p>
<p>As usual, we approach this task gradually, starting with a background section that describes how binary codes and Boolean arithmetic can be used, respectively, to represent and add signed integers. The Specification section presents a succession of <i>adder chips</i> designed to add two bits, three bits, and pairs of <i>n</i>-bit binary numbers. This sets the stage for the ALU specification, which is based on a surprisingly simple logic design. The Implementation and Project sections provide tips and guidelines on how to build the adder chips and the ALU using HDL and the supplied hardware simulator.</p>
<section epub:type="division">
<h2 class="head a-head"><b>2.1    Arithmetic Operations</b></h2>
<p class="noindent">General-purpose computer systems are required to perform at least the following arithmetic operations on signed integers:</p>
<ul class="List-1">
<li class="BLF">addition</li>
<li class="BL1">sign conversion</li>
<li class="BL1">subtraction</li>
<li class="BL1">comparison</li>
<li class="BL1">multiplication</li>
<li class="BLL1">division</li>
</ul>
<p class="TNI1"><span aria-label="32" id="pg_32" role="doc-pagebreak"/>We’ll start by developing gate logic that carries out addition and sign conversion. Later, we will show how the other arithmetic operations can be implemented from these two building blocks.</p>
<p>In mathematics as well as in computer science, <i>addition</i> is a simple operation that runs deep. Remarkably, all the functions performed by digital computers—not only arithmetic operations—can be reduced to adding binary numbers. Therefore, constructive understanding of binary addition holds the key to understanding many fundamental operations performed by the computer’s hardware.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.2    Binary Numbers</b></h2>
<p class="noindent">When we are told that a certain code, say, 6083, represents a number using the <i>decimal system</i>, then, by convention, we take this number to be:</p>
<figure class="IMG"><img alt="" class="inline" height="16" src="../images/2-1.png" width="318"/></figure>
<p class="TNI1">Each digit in the decimal code contributes a value that depends on the <i>base</i> 10 and on the digit’s position in the code. Suppose now that we are told that the code 10011 represents a number using base 2, or <i>binary</i> representation. To compute the value of this number, we follow exactly the same procedure, using base 2 instead of base 10:</p>
<figure class="IMG"><img alt="" class="inline" height="16" src="../images/2-2.png" width="331"/></figure>
<p class="TNI1">Inside computers, <i>everything</i> is represented using binary codes. For example, when we press the keyboard keys labeled <samp class="SANS_Consolas_Regular_11">1</samp>, <samp class="SANS_Consolas_Regular_11">9</samp>, and <samp class="SANS_Consolas_Regular_11">Enter</samp> in response to “Give an example of a prime number,” what ends up stored in the computer’s memory is the binary code <samp class="SANS_Consolas_Regular_11">10011</samp>. When we ask the computer to display this value on the screen, the following process ensues. First, the computer’s operating system calculates the decimal value that <samp class="SANS_Consolas_Regular_11">10011</samp> represents, which happens to be 19. After converting this integer value to the two characters <samp class="SANS_Consolas_Regular_11">1</samp> and <samp class="SANS_Consolas_Regular_11">9</samp>, the OS looks up the current font and gets the two bitmap images used for rendering these characters on the screen. The OS then causes the screen driver to turn on and off the relevant pixels, and, don’t hold your breath—the whole thing lasts a tiny fraction of a second—we finally see the image <samp class="SANS_Consolas_Regular_11">19</samp> appear on the screen.</p>
<p>In chapter 12 we’ll develop an operating system that carries out such rendering operations, among many other low-level services. For now, suffice it to observe that the decimal representation of numbers is a human indulgence explained by the obscure fact that, at some point in ancient history, humans decided to represent quantities using their ten fingers, and the habit stuck. From a mathematical perspective, the number ten is utterly uninteresting, and, as far as computers go, is a complete nuisance. Computers handle <i>everything</i> in binary and care naught about decimal. Yet since humans insist on dealing with numbers using decimal codes, computers have to work hard behind the scenes to carry out binary-to-decimal and decimal-to-binary conversions whenever humans want to see, or supply, numeric information. At all other times, computers stick to binary.</p>
<p class="STNI1"><span aria-label="33" id="pg_33" role="doc-pagebreak"/><b>Fixed word size</b>: Integer numbers are of course unbounded: for any given number <i>x</i> there are integers that are less than <i>x</i> and integers that are greater than <i>x</i>. Yet computers are finite machines that use a fixed word size for representing numbers. <i>Word size</i> is a common hardware term used for specifying the number of bits that computers use for representing a basic chunk of information—in this case, integer values. Typically, 8-, 16-, 32-, or 64-bit registers are used for representing integers.<sup><a href="#footnote-003" id="footnote-003-backlink" role="doc-noteref">1</a></sup> The fixed word size implies that there is a limit on the number of values that these registers can represent.</p>
<p>For example, suppose we use 8-bit registers for representing integers. This representation can code <img alt="" class="inline" height="13" src="../images/2-3.png" width="54"/> different things. If we wish to represent only nonnegative integers, we can assign <samp class="SANS_Consolas_Regular_11">00000000</samp> for representing 0, <samp class="SANS_Consolas_Regular_11">00000001</samp> for representing 1, <samp class="SANS_Consolas_Regular_11">00000010</samp> for representing 2, <samp class="SANS_Consolas_Regular_11">00000011</samp> for representing 3, all the way up to assigning <samp class="SANS_Consolas_Regular_11">11111111</samp> for representing 255. In general, using <i>n</i> bits we can represent all the nonnegative integers ranging from 0 to <img alt="" class="inline" height="12" src="../images/2-4.png" width="34"/>.</p>
<p>What about representing negative numbers using binary codes? Later in the chapter we’ll present a technique that meets this challenge in a most elegant and satisfying way.</p>
<p>And what about representing numbers that are greater than, or less than, the maximal and minimal values permitted by the fixed register size? Every high-level language provides abstractions for handling numbers that are as large or as small as we can practically want. These abstractions are typically implemented by lashing together as many <i>n</i>-bit registers as necessary for representing the numbers. Since executing arithmetic and logical operations on multi-word numbers is a slow affair, it is recommended to use this practice only when the application requires processing extremely large or extremely small numbers.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.3    Binary Addition</b></h2>
<p class="noindent">A pair of binary numbers can be added bitwise from right to left, using the same decimal addition algorithm learned in elementary school. First, we add the two rightmost bits, also called the <i>least significant bits</i> (LSB) of the two binary numbers. Next, we add the resulting carry bit to the sum of the next pair of bits. We continue this lockstep process until the two left <i>most significant bits</i> (MSB) are added. Here is an example of this algorithm in action, assuming that we use a fixed word size of 4 bits:</p>
<figure class="IMG-L"><img alt="" class="img100" src="../images/figure_wo_caption_2.1.png"/></figure>
<p><span aria-label="34" id="pg_34" role="doc-pagebreak"/>If the most significant bitwise addition generates a carry of 1, we have what is known as <i>overflow</i>. What to do with overflow is a matter of decision, and ours is to ignore it. Basically, we are content to guarantee that the result of adding any two <i>n</i>-bit numbers will be correct up to <i>n</i> bits. We note in passing that ignoring things is perfectly acceptable as long as one is clear and forthcoming about it.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.4    Signed Binary Numbers</b></h2>
<p class="TNI1">An <i>n</i>-bit binary system can code 2<i><sup>n</sup></i> different things. If we have to represent signed (positive and negative) numbers in binary code, a natural solution is to split the available code space into two subsets: one for representing nonnegative numbers, and the other for representing negative numbers. Ideally, the coding scheme should be chosen such that the introduction of signed numbers would complicate the hardware implementation of arithmetic operations as little as possible.</p>
<p>Over the years, this challenge has led to the development of several coding schemes for representing signed numbers in binary code. The solution used today in almost all computers is called the <i>two’s complement</i> method, also known as <i>radix complement</i>. In a binary system that uses a word size of <i>n</i> bits, the two’s complement binary code that represents negative <i>x</i> is taken to be the code that represents <img alt="" class="inline" height="12" src="../images/2-5.png" width="39"/>. For example, in a 4-bit binary system, <span class="symb"><img alt="" class="inline" height="11" src="../images/2-6.png" width="16"/></span> is represented using the binary code associated with <img alt="" class="inline" height="13" src="../images/2-7.png" width="64"/>, which happens to be <samp class="SANS_Consolas_Regular_11">1001</samp>. Recalling that <img alt="" class="inline" height="11" src="../images/2-8.png" width="17"/> is represented by <samp class="SANS_Consolas_Regular_11">0111</samp>, we see that <samp class="SANS_Consolas_Regular_11"><img alt="" class="inline" height="10" src="../images/2-9C.png" width="128"/></samp> (ignoring the overflow bit). <a href="chapter_2.xhtml#fig2-1" id="rfig2-1">Figure 2.1</a> lists all the signed numbers represented by a 4-bit system using the two’s complement method.</p>
<figure class="IMG"><img alt="" height="550" id="fig2-1" src="../images/figure_2.1.png"/>
<figcaption><p class="CAP"><b><a href="#rfig2-1">Figure 2.1</a></b>    Two’s complement representation of signed numbers, in a 4-bit binary system.</p></figcaption>
</figure>
<p>An inspection of <a href="chapter_2.xhtml#fig2-1">figure 2.1</a> suggests that an <i>n</i>-bit binary system with two’s complement representation has the following attractive properties:</p>
<ul class="List-1">
<li class="BLF">The system codes <img alt="" class="inline" height="12" src="../images/2-12.png" width="13"/> signed numbers, ranging from <span class="symb"><img alt="" class="inline" height="15" src="../images/2-10.png" width="46"/></span> to <img alt="" class="inline" height="14" src="../images/2-11.png" width="51"/></li>
<li class="BL1">The code of any nonnegative number begins with a <samp class="SANS_Consolas_Regular_11">0</samp>.</li>
<li class="BL1">The code of any negative number begins with a <samp class="SANS_Consolas_Regular_11">1</samp>.</li>
<li class="BLL1">To obtain the binary code of <span class="symb"><img alt="" class="inline" height="8" src="../images/2-13.png" width="18"/></span> from the binary code of <i>x</i>, leave all the least significant <samp class="SANS_Consolas_Regular_11">0</samp>-bits and the first least significant <samp class="SANS_Consolas_Regular_11">1</samp>-bit of <i>x</i> intact, and flip all the remaining bits (convert <samp class="SANS_Consolas_Regular_11">0</samp>’s to <samp class="SANS_Consolas_Regular_11">1</samp>’s and vice versa). Alternatively, flip all the bits of <i>x</i> and add 1 to the result.</li>
</ul>
<p class="TNI1">A particularly attractive feature of the two’s complement representation is that <i>subtraction</i> is handled as a special case of addition. To illustrate, consider <img alt="" class="inline" height="12" src="../images/2-14.png" width="36"/> Noting that this is equivalent to <img alt="" class="inline" height="14" src="../images/2-15.png" width="57"/> and following <a href="chapter_2.xhtml#fig2-1">figure 2.1</a>, we proceed to compute <samp class="SANS_Consolas_Regular_11"><img alt="" class="inline" height="10" src="../images/2-16.png" width="83"/></samp> The result is <samp class="SANS_Consolas_Regular_11">1110</samp>, which indeed is the binary code of <span class="symb"><img alt="" class="inline" height="12" src="../images/2-19.png" width="17"/></span>. Here is another example: To compute <img alt="" class="inline" height="14" src="../images/2-17.png" width="82"/> we add <img alt="" class="inline" height="12" src="../images/2-18.png" width="83"/> obtaining the sum <samp class="SANS_Consolas_Regular_11">11011</samp>. Ignoring the overflow bit, we get <samp class="SANS_Consolas_Regular_11">1011</samp>, which is the binary code of <span class="symb"><img alt="" class="inline" height="12" src="../images/2-20.png" width="20"/></span></p>
<p>We see that the two’s complement method enables adding and subtracting signed numbers using nothing more than the hardware required for adding nonnegative numbers. <span aria-label="35" id="pg_35" role="doc-pagebreak"/>As we will see later in the book, every arithmetic operation, from multiplication to division to square root, can be implemented reductively using binary addition. So, on the one hand, we observe that a huge range of computer capabilities rides on top of binary addition, and on the other hand, we observe that the two’s complement method obviates the need for special hardware for adding and subtracting signed numbers. Taking these two observations together, we are compelled to conclude that the two’s complement method is one of the most remarkable and unsung heroes of applied computer science.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.5    Specification</b></h2>
<p class="noindent">We now turn to specifying a hierarchy of chips, starting with simple adders and culminating with an Arithmetic Logic Unit (ALU). As usual in this book, we focus first on the abstract (<i>what</i> the chips are designed to), delaying implementation details (<i>how</i> they do it) to the next section. We cannot resist reiterating, with pleasure, that thanks to the two’s complement method we don’t have to say anything special about handling signed numbers. All the arithmetic chips that we’ll present work equally well on nonnegative, negative, and mixed-sign numbers.</p>
<section epub:type="division">
<h3 class="head b-head"><b>2.5.1    Adders</b></h3>
<p class="noindent">We’ll focus on the following hierarchy of <i>adders</i>:</p>
<ul class="List-1">
<li class="BLF"><i>Half-adder</i>: designed to add two bits</li>
<li class="BL1"><i>Full-adder</i>: designed to add three bits</li>
<li class="BLL1"><i>Adder</i>: designed to add two <i>n</i>-bit numbers</li>
</ul>
<p class="TNI1"><span aria-label="36" id="pg_36" role="doc-pagebreak"/>We’ll also specify a special-purpose adder, called an <i>incrementer</i>, designed to add 1 to a given number. (The names <i>half-adder</i> and <i>full-adder</i> derive from the implementation detail that a full-adder chip can be realized from two half-adders, as we’ll see later in the chapter.)</p>
<p class="STNI1"><b>Half-adder</b>: The first step on our road to adding binary numbers is adding two bits. Let us treat the result of this operation as a 2-bit number, and call its right and left bits <samp class="SANS_Consolas_Regular_11">sum</samp> and <samp class="SANS_Consolas_Regular_11">carry</samp>, respectively. <a href="chapter_2.xhtml#fig2-2" id="rfig2-2">Figure 2.2</a> presents a chip that carries out this addition operation.</p>
<figure class="IMG"><img alt="" id="fig2-2" src="../images/figure_2.2.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig2-2">Figure 2.2</a></b>    Half-adder, designed to add 2 bits.</p></figcaption>
</figure>
<p class="STNI1"><b>Full-adder</b>: <a href="chapter_2.xhtml#fig2-3" id="rfig2-3">Figure 2.3</a> presents a <i>full-adder</i> chip, designed to add three bits. Like the half-adder, the full-adder chip outputs two bits that, taken together, represent the addition of the three input bits.</p>
<figure class="IMG"><img alt="" id="fig2-3" src="../images/figure_2.3.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig2-3">Figure 2.3</a></b>    Full-adder, designed to add 3 bits.</p></figcaption>
</figure>
<p class="STNI1"><b>Adder</b>: Computers represent integer numbers using a fixed word size like 8, 16, 32, or 64 bits. The chip whose job is to add two such <i>n</i>-bit numbers is called <i>adder</i>. <a href="chapter_2.xhtml#fig2-4" id="rfig2-4">Figure 2.4</a> presents a 16-bit adder.</p>
<figure class="IMG"><img alt="" id="fig2-4" src="../images/figure_2.4.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig2-4">Figure 2.4</a></b>    16-bit adder, designed to add two 16-bit numbers, with an example of addition action (on the left).</p></figcaption>
</figure>
<p>We note in passing that the logic design for adding 16-bit numbers can be easily extended to implement any <i>n</i>-bit adder chip, irrespective of <i>n</i>.</p>
<p class="STNI1"><b>Incrementer</b>: When we later design our computer architecture, we will need a chip that adds 1 to a given number (<i>Spoiler</i>: This will enable fetching the next instruction from memory, after executing the current one). Although the <img alt="" class="inline" height="12" src="../images/2-21.png" width="30"/> operation can be realized by our general-propose <samp class="SANS_Consolas_Regular_11">Adder</samp> chip, a dedicated <i>incrementer</i> chip can do it more efficiently. Here is the chip interface:</p>
<figure class="IMG-L"><img alt="" class="img100" src="../images/figure_wo_caption_2.2.png"/></figure>
</section>
<section epub:type="division">
<h3 class="head b-head"><b>2.5.2    The Arithmetic Logic Unit</b></h3>
<p class="noindent">All the adder chips presented so far are generic: any computer that performs arithmetic operations uses such chips, one way or another. Building on these chips, we now turn to describe an <i>Arithmetic Logic Unit</i>, a chip that will later become the computational centerpiece of our CPU. Unlike the generic gates and chips discussed thus far, the ALU design is unique to the computer built in Nand to Tetris, named Hack. That said, the design principles underlying the Hack ALU are general and instructive. Further, our ALU architecture achieves a great deal of functionality using a minimal set of internal parts. In that respect, it provides a good example of an efficient and elegant logic design.</p>
<p><span aria-label="37" id="pg_37" role="doc-pagebreak"/>As its name implies, an Arithmetic Logic Unit is a chip designed to compute a set of arithmetic and logic operations. Exactly <i>which</i> operations an ALU should feature is a design decision derived from cost-effectiveness considerations. In the case of the Hack platform, we decided that (i) the ALU will perform only integer arithmetic (and not, for example, floating point arithmetic) and (ii) the ALU will compute the set of eighteen arithmetic-logical functions shown in <a href="chapter_2.xhtml#fig2-5a" id="rfig2-5a">figure 2.5a</a>.</p>
<figure class="IMG"><img alt="" id="fig2-5a" src="../images/figure_2.5a.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig2-5a">Figure 2.5a</a></b>    The Hack ALU, designed to compute the eighteen arithmetic-logical functions shown on the right (the symbols <samp class="SANS_Consolas_Regular_11">!</samp>, <samp class="SANS_Consolas_Regular_11">&amp;</samp>, and <samp class="SANS_Consolas_Regular_11">|</samp> represent, respectively, the 16-bit operations <samp class="SANS_Consolas_Regular_11">Not</samp>, <samp class="SANS_Consolas_Regular_11">And</samp>, and <samp class="SANS_Consolas_Regular_11">Or</samp>). For now, ignore the <samp class="SANS_Consolas_Regular_11">zr</samp> and <samp class="SANS_Consolas_Regular_11">ng</samp> output bits.</p></figcaption>
</figure>
<p><span aria-label="38" id="pg_38" role="doc-pagebreak"/>As seen in <a href="chapter_2.xhtml#fig2-5a">figure 2.5a</a>, the Hack ALU operates on two 16-bit two’s complement integers, denoted <samp class="SANS_Consolas_Regular_11">x</samp> and <samp class="SANS_Consolas_Regular_11">y</samp>, and on six 1-bit inputs, called <i>control bits</i>. These control bits “tell” the ALU which function to compute. The exact specification is given in <a href="chapter_2.xhtml#fig2-5b" id="rfig2-5b">figure 2.5b</a>.</p>
<figure class="IMG"><img alt="" height="550" id="fig2-5b" src="../images/figure_2.5b.png"/>
<figcaption><p class="CAP"><b><a href="#rfig2-5b">Figure 2.5b</a></b>    Taken together, the values of the six control bits <samp class="SANS_Consolas_Regular_11">zx</samp>, <samp class="SANS_Consolas_Regular_11">nx</samp>, <samp class="SANS_Consolas_Regular_11">zy</samp>, <samp class="SANS_Consolas_Regular_11">ny</samp>, <samp class="SANS_Consolas_Regular_11">f</samp>, and <samp class="SANS_Consolas_Regular_11">no</samp> cause the ALU to compute one of the functions listed in the rightmost column.</p></figcaption>
</figure>
<p>To illustrate the ALU logic, suppose we wish to compute the function <img alt="" class="inline" height="14" src="../images/2-22.png" width="35"/> for <i><img alt="" class="inline" height="12" src="../images/2-23.png" width="45"/></i> To get started, we feed the 16-bit binary code of 27 into the <samp class="SANS_Consolas_Regular_11">x</samp> input. In this particular example we don’t care about <samp class="SANS_Consolas_Regular_11">y</samp>’s value, since it has no impact on the required calculation. Now, looking up <img alt="" class="inline" height="10" src="../images/2-C2.png" width="28"/> in <a href="chapter_2.xhtml#fig2-5b">figure 2.5b</a>, we set the ALU’s control bits to <samp class="SANS_Consolas_Regular_11">001110</samp>. According to the specification, this setting should cause the ALU to output the binary code representing 26.</p>
<p>Is that so? To find out, let’s delve deeper, and reckon how the Hack ALU performs its magic. Focusing on the top row of <a href="chapter_2.xhtml#fig2-5b">figure 2.5b</a>, note that each one of the six control bits <span aria-label="39" id="pg_39" role="doc-pagebreak"/>is associated with a standalone, conditional micro-action. For example, the <samp class="SANS_Consolas_Regular_11">zx</samp> bit is associated with “if (<samp class="SANS_Consolas_Regular_11">zx</samp><span class="code_eq-symb">==</span><samp class="SANS_Consolas_Regular_11">1</samp>) then set <samp class="SANS_Consolas_Regular_11">x</samp> to <samp class="SANS_Consolas_Regular_11">0</samp>”. These six directives are to be performed in order: first, we either set the <samp class="SANS_Consolas_Regular_11">x</samp> and <samp class="SANS_Consolas_Regular_11">y</samp> inputs to <samp class="SANS_Consolas_Regular_11">0</samp>, or not; next, we either negate the resulting values, or not; next, we compute either <span class="symb-code">+</span> or <samp class="SANS_Consolas_Regular_11">&amp;</samp> on the preprocessed values; and finally, we either negate the resulting value, or not. All these settings, negations, additions, and conjunctions are 16-bit operations.</p>
<p>With this logic in mind, let us revisit the row associated with <samp class="SANS_Consolas_Regular_11">x-1</samp> and verify that the micro-operations coded by the six control bits will indeed cause the ALU to compute <i><img alt="" class="inline" height="12" src="../images/2-24.png" width="35"/></i> Going left to right, we see that the <samp class="SANS_Consolas_Regular_11">zx</samp> and <samp class="SANS_Consolas_Regular_11">nx</samp> bits are <samp class="SANS_Consolas_Regular_11">0</samp>, so we neither zero nor negate the <samp class="SANS_Consolas_Regular_11">x</samp> input—we leave it as is. The <samp class="SANS_Consolas_Regular_11">zy</samp> and <samp class="SANS_Consolas_Regular_11">ny</samp> bits are 1, so we first zero the <samp class="SANS_Consolas_Regular_11">y</samp> input and then negate the result, yielding the 16-bit value <samp class="SANS_Consolas_Regular_11">1111111111111111</samp>. Since this binary code happens to represent <span class="symb"><img alt="" class="inline" height="12" src="../images/2-25.png" width="16"/></span> in two’s complement, we see that the two data inputs of the ALU <span aria-label="40" id="pg_40" role="doc-pagebreak"/>are now <samp class="SANS_Consolas_Regular_11">x</samp>’s value and <span class="symb-code"><img alt="" class="inline" height="10" src="../images/2-C1.png" width="15"/></span>. Since the <samp class="SANS_Consolas_Regular_11">f</samp> bit is <samp class="SANS_Consolas_Regular_11">1</samp>, the selected operation is <i>addition</i>, causing the ALU to compute <i><img alt="" class="inline" height="14" src="../images/2-27.png" width="55"/></i> Finally, since the <samp class="SANS_Consolas_Regular_11">no</samp> bit is <samp class="SANS_Consolas_Regular_11">0</samp>, the output is not negated. To conclude, we’ve illustrated that if we feed the ALU with <samp class="SANS_Consolas_Regular_11">x</samp> and <samp class="SANS_Consolas_Regular_11">y</samp> values and set the six control bits to <samp class="SANS_Consolas_Regular_11">001110</samp>, the ALU will compute <i><img alt="" class="inline" height="14" src="../images/2-28.png" width="35"/></i> as specified.</p>
<p>What about the other seventeen functions listed in <a href="chapter_2.xhtml#fig2-5b">figure 2.5b</a>? Does the ALU actually compute them as well? To verify that this is indeed the case, you are invited to focus on other rows in the table, go through the same process of carrying out the micro-actions coded by the six control bits, and figure out for yourself what the ALU will output. Or, you can believe us that the ALU works as advertised.</p>
<p>Note that the ALU actually computes a total of sixty-four functions, since six control bits code that many possibilities. We’ve decided to focus on, and document, only eighteen of these possibilities, since these will suffice for supporting the instruction set of our target computer system. The curious reader may be intrigued to know that some of the undocumented ALU operations are quite meaningful. However, we’ve opted not to exploit them in the Hack system.</p>
<p>The Hack ALU interface is given in <a href="chapter_2.xhtml#fig2-5c" id="rfig2-5c">figure 2.5c</a>. Note that in addition to computing the specified function on its two inputs, the ALU also computes the two output bits <samp class="SANS_Consolas_Regular_11">zr</samp> and <samp class="SANS_Consolas_Regular_11">ng</samp>. These bits, which flag whether the ALU output is zero or negative, respectively, will be used by the future CPU of our computer system.</p>
<figure class="IMG"><img alt="" id="fig2-5c" src="../images/figure_2.5c.png" width="450"/>
<figcaption><p class="CAP"><b><a href="#rfig2-5c">Figure 2.5c</a></b>    The Hack ALU API.</p></figcaption>
</figure>
<p><span aria-label="41" id="pg_41" role="doc-pagebreak"/>It may be instructive to describe the thought process that led to the design of our ALU. First, we made a tentative list of the primitive operations that we wanted our computer to perform (right column of <a href="chapter_2.xhtml#fig2-5b">figure 2.5b</a>). Next, we used backward reasoning to figure out how <samp class="SANS_Consolas_Regular_11">x</samp>, <samp class="SANS_Consolas_Regular_11">y</samp>, and <samp class="SANS_Consolas_Regular_11">out</samp> can be manipulated in binary fashion in order to carry out the desired operations. These processing requirements, along with our objective to keep the ALU logic as simple as possible, have led to the design decision to use six control bits, each associated with a straightforward operation that can be easily implemented with basic logic gates. The resulting ALU is simple and elegant. And in the hardware business, simplicity and elegance carry the day.</p>
</section>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.6    Implementation</b></h2>
<p class="noindent">Our implementation guidelines are intentionally minimal. We already gave many implementation tips along the way, and now it is your turn to discover the missing parts in the chip architectures.</p>
<p>Throughout this section, when we say “build/implement a logic design that <span class="ellipsis">…</span>,” we expect you to (i) figure out the logic design (e.g., by sketching a gate diagram), (ii) write the HDL code that realizes the design, and (iii) test and debug your design using the supplied test scripts and hardware simulator. More details are given in the next section, which describes project 2.</p>
<p class="STNI1"><b>Half-adder</b>: An inspection of the truth table in <a href="chapter_2.xhtml#fig2-2">figure 2.2</a> reveals that the outputs <samp class="SANS_Consolas_Regular_11">su</samp><samp class="SANS_Consolas_Regular_11">m</samp><samp class="SANS_Consolas_Regular_11">(a,b)</samp> and <samp class="SANS_Consolas_Regular_11">carr</samp><samp class="SANS_Consolas_Regular_11">y</samp><samp class="SANS_Consolas_Regular_11">(a,b)</samp> happen to be identical to those of two simple Boolean functions discussed and implemented in project 1. Therefore, the half-adder implementation is straightforward.</p>
<p class="STNI1"><b>Full-adder</b>: A full-adder chip can be implemented from two half-adders and one additional gate (and that’s why these adders are called <i>half</i> and <i>full)</i>. Other implementations are possible, including direct approaches that don’t use half-adders.</p>
<p class="STNI1"><b>Adder</b>: The addition of two <i>n</i>-bit numbers can be done bitwise, from right to left. In step 0, the least significant pair of bits is added, and the resulting carry bit is fed into the addition of the next significant pair of bits. The process continues until the pair of the most significant bits is added. Note that each step involves the addition of three bits, one of which is propagated from the “previous” addition.</p>
<p>Readers may wonder how we can add pairs of bits “in parallel” before the carry bit has been computed by the previous pair of bits. The answer is that these computations are sufficiently fast as to complete and stabilize within one clock cycle. We’ll discuss clock cycles and synchronization in the next chapter; for now, you can ignore the time element completely, and write HDL code that computes the addition operation by acting on all the bit-pairs simultaneously.</p>
<p class="STNI1"><span aria-label="42" id="pg_42" role="doc-pagebreak"/><b>Incrementer</b>: An <i>n</i>-bit incrementer can be implemented easily in a number of different ways.</p>
<p class="STNI1"><b>ALU</b>: Our ALU was carefully planned to effect all the desired ALU operations <i>logically</i>, using the simple Boolean operations implied by the six control bits. Therefore, the <i>physical</i> implementation of the ALU can be reduced to implementing these simple operations, following the pseudocode specifications listed at the top of <a href="chapter_2.xhtml#fig2-5b">figure 2.5b</a>. Your first step will likely be creating a logic design for zeroing and negating a 16-bit value. This logic can be used for handling the <samp class="SANS_Consolas_Regular_11">x</samp> and <samp class="SANS_Consolas_Regular_11">y</samp> inputs as well as the <samp class="SANS_Consolas_Regular_11">out</samp> output. Chips for bitwise And-ing and addition have already been built in projects 1 and 2, respectively. Thus, what remains is building logic that selects between these two operations, according to the <samp class="SANS_Consolas_Regular_11">f</samp> control bit (this selection logic was also implemented in project 1). Once this main ALU functionality works properly, you can proceed to implement the required functionality of the single-bit <samp class="SANS_Consolas_Regular_11">zr</samp> and <samp class="SANS_Consolas_Regular_11">ng</samp> outputs.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><b>2.7    Project</b></h2>
<p class="noindent"><b>Objective</b>: Implement all the chips presented in this chapter. The only building blocks that you need are some of the gates described in chapter 1 and the chips that you will gradually build in this project.</p>
<p class="STNI1"><b>Built-in chips</b>: As was just said, the chips that you will build in this project use, as chip-parts, some of the chips described in chapter 1. Even if you’ve built these lower-level chips successfully in HDL, we recommend using their built-in versions instead. As best-practice advice pertaining to all the hardware projects in Nand to Tetris, always prefer using built-in chip-parts instead of their HDL implementations. The built-in chips are guaranteed to operate to specification and are designed to speed up the operation of the hardware simulator.</p>
<p>There is a simple way to follow this best-practice advice: Don’t add to the project folder <samp class="SANS_Consolas_Regular_11">nand2tetris/projects/02</samp> any <samp class="SANS_Consolas_Regular_11">.hdl</samp> file from project 1. Whenever the hardware simulator will encounter in your HDL code a reference to a chip-part from project 1, for example, <samp class="SANS_Consolas_Regular_11">And16</samp>, it will check whether there is an <samp class="SANS_Consolas_Regular_11">And16.hdl</samp> file in the current folder. Failing to find it, the hardware simulator will resort by default to using the built-in version of this chip, which is exactly what we want.</p>
<p>The remaining guidelines for this project are identical to those of project 1. In particular, remember that good HDL programs use as few chip-parts as possible, and there is no need to invent and implement any “helper chips”; your HDL programs should use only chips that were specified in chapters 1 and 2.</p>
<p class="STNI1"><b>A web-based version of project 2</b> is available at <a href="http://www.nand2tetris.org">www<wbr/>.nand2tetris<wbr/>.org</a>.</p>
</section>
<section epub:type="division">
<h2 class="head a-head"><span aria-label="43" id="pg_43" role="doc-pagebreak"/><b>2.8    Perspective</b></h2>
<p class="noindent">The construction of the multi-bit adder presented in this chapter was standard, although no attention was paid to efficiency. Indeed, our suggested adder implementation is inefficient, due to the delays incurred while the carry bits propagate throughout the <i>n</i>-bit addends. This computation can be accelerated by logic circuits that effect so-called <i>carry lookahead</i> heuristics. Since addition is the most prevalent operation in computer architectures, any such low-level improvement can result in dramatic performance gains throughout the system. Yet in this book we focus mostly on functionality, leaving chip optimization to more specialized hardware books and courses.<sup><a href="#footnote-002" id="footnote-002-backlink" role="doc-noteref">2</a></sup></p>
<p>The overall functionality of any hardware/software system is delivered jointly by the CPU and the operating system that runs on top of the hardware platform. Thus, when designing a new computer system, the question of how to allocate the desired functionality between the ALU and the OS is essentially a cost/performance dilemma. As a rule, direct hardware implementations of arithmetic and logical operations are more efficient than software implementations but make the hardware platform more expensive.</p>
<p>The trade-off that we have chosen in Nand to Tetris is to design a basic ALU with minimal functionality, and use system software to implement additional mathematical operations as needed. For example, our ALU features neither multiplication nor division. In part II of the book, when we discuss the operating system (chapter 12), we’ll implement elegant and efficient bitwise algorithms for multiplication and division, along with other mathematical operations. These OS routines can then be used by compilers of high-level languages operating on top of the Hack platform. Thus, when a high-level programmer writes an expression like, say, <img alt="" class="inline" height="14" src="../images/43.png" width="112"/> then, following compilation, some parts of the expression will be evaluated directly by the ALU and some by the OS, yet the high-level programmer will be completely oblivious to this low-level division of work. Indeed, one of the key roles of an operating system is closing gaps between the high-level language abstractions that programmers use and the barebone hardware on which they are realized.</p>
<hr class="HorizontalRule-1"/>
<ol class="footnotes">
<li><p class="FN" role="doc-footnote"><span class="fnnum"><a href="#footnote-003-backlink" id="footnote-003">1</a></span>.  Which correspond, respectively, to the typical high-level data types <i>byte</i>, <i>short</i>, <i>int</i>, and <i>long</i>. For example, when reduced to machine-level instructions, <i>short</i> variables can be handled by 16-bit registers. Since 16-bit arithmetic is four times faster than 64-bit arithmetic, programmers are advised to always use the most compact data type that satisfies the application’s requirements.</p></li>
<li><p class="FN" role="doc-footnote"><span class="fnnum"><a href="#footnote-002-backlink" id="footnote-002">2</a></span>.  A technical reason for not using carry look-ahead techniques in our adder chips is that their hardware implementation requires cyclical pin connections, which are not supported by the Nand to Tetris hardware simulator.</p></li>
</ol>
</section>
</section>
</div>
</body>
</html>